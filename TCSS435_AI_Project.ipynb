{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCSS 435 - Class Project: Can AI Think Like a Human?\n",
    "Johan Hernandez\n",
    "### Due: June 10th, 2025\n",
    "---\n",
    "## Step 1: Understanding LLMs & Transformers (20 pts)\n",
    "- Explain how large language models are trained and how transformers\n",
    "work\n",
    "    - Large language models are trained using self-supervised learning on huge amounts of textual data. The first step in this process is generally to collect massive amounts of data from books, websites, wikipedia, and other textual sources. A large language model is then trained to predict the next token in a sequence of tokens, for example:\n",
    "        ```txt\n",
    "        Input: \"I brushed my teeth with\"\n",
    "        Target: \"Toothpaste\"\n",
    "        ```\n",
    "        The LLM uses a transformer neural network where up to billions of parameters are adjusted through gradient descent and backpropagation to minimize error in predictions. Lastly, training requires high-performance GPUs or TPUs to train such a large model with high amounts of parameters and data, as training is resource intensive.\n",
    "    - Transformers are the architecture behind most modern LLMs, as mentioned in the paper \"Attention Is All You Need\" by Ashish Vaswani et al. This architecture made handling context more efficiently and revolutionized natural language procesing. A transformer has input embedding, which converts words (tokens) into vectors using an embedding layer. They use positional encoding to know the order of the words, since they don't use recurrence like RNNs. The core mechanism of a transformer is multi-head attention, which allows the model to look at all other words in the input and weigh their relevance to the current word (see diagram and explanation below). After attention, the output is passed through a feedforward neural network to transform the representations. Transformers stack these layers many times to improve accuracy, for example GPT-3 uses 96+ layers \n",
    "\n",
    "- Include a diagram of a transformer model and explain 'attention'\n",
    "    - The following is a diagram of the transformer model from \"Attention Is All You Need\" (Vaswani et al. 2017):\n",
    "\n",
    "    ![Diagram of the transformer model](images/transformer-model.png)\n",
    "\n",
    "    - Attention is what a LLM uses to know the relevance of tokens when compared with other tokens, or in other words what tokens to \"pay attention\" to. An attention function is described as \"mapping a query and a set of key-value pairs to an output, where teh query, keys, and output are all vectors\" (Vaswani et al. 2017). The following is a diagram escribing an attention funtion:\n",
    "\n",
    "    ![Diagram of the attention model](images/attention-model.png)\n",
    "    \n",
    "    For each token, a model computes:\n",
    "    $\n",
    "    Attention(Q, K, V) = softmax(QK^T / \\sqrt{d_k}) * V\n",
    "    $\n",
    "    Where Q is the query, K is the key, V is the value, and d_k is the dimension of key vectors. This formula weights how relevant each word is to the current one.\n",
    "\n",
    "- Give 2 examples of real-world LLM applications\n",
    "    - One real world example of an LLM is a coding assistant like GitHub Copilot. These models can generate code, explain logic, and autocomplete functions for developers. These LLMs specialize in not only natural language explanations, but also require a great deal of attention to code bases when dealing with coding suggestions and autocompletions.\n",
    "    - Another real world LLM example is a customer support chatbot. These LLMs handle simple questions from customers about a company's products or services in natural language responses, unlike coding assistants.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Explore the CommonsenseQA Dataset (15 pts)\n",
    "- Load dataset\n",
    "- Print 5 random samples\n",
    "- Identify commonsense types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/johan/.pyenv/versions/ll/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Question: If someone is clinging on to life they can be in a what?\n",
      "Choices: ['ending soon', 'ill', 'death', 'coma', 'void']\n",
      "Answer: D\n",
      "\n",
      "Sample 2:\n",
      "Question: Who is someone competing against?\n",
      "Choices: ['effort', 'time', 'opponent', 'skill', 'competition']\n",
      "Answer: C\n",
      "\n",
      "Sample 3:\n",
      "Question: The real estate alienated people, what did she need to learn to be?\n",
      "Choices: ['deceive', 'charming', 'manipulate', 'lie', 'train']\n",
      "Answer: B\n",
      "\n",
      "Sample 4:\n",
      "Question: Where do tourists frequent most in mexico?\n",
      "Choices: ['beach', 'zoo', 'waterfall', 'homes', 'disneyland']\n",
      "Answer: A\n",
      "\n",
      "Sample 5:\n",
      "Question: what does someone getting in shape want to achieve?\n",
      "Choices: ['exercise', 'loss of muscle', 'losing weight', 'good health', 'sweat']\n",
      "Answer: D\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "# Load CommonsenseQA dataset from Hugging Face\n",
    "dataset = load_dataset(\"commonsense_qa\")\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Display 5 random samples\n",
    "samples = random.sample(list(train_data), 5)\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f\"Sample {i+1}:\\nQuestion: {sample['question']}\\nChoices: {sample['choices']['text']}\\nAnswer: {sample['answerKey']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build and Evaluate Baseline Models (15 pts)\n",
    "- Random model\n",
    "- Fixed-choice model\n",
    "- Accuracy over 100 questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: The condominium has public recreational areas, what was the purpose of these?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: E\n",
      "Correct Answer: E\n",
      "\n",
      "Question: If I had more than one steel pen (e.g. 100,000), where would I store it?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: C\n",
      "Correct Answer: B\n",
      "\n",
      "Question: The machine was very intricate, it was quite an what?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: E\n",
      "Correct Answer: B\n",
      "\n",
      "Question: Killing will result in what kind of trial?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: D\n",
      "Correct Answer: A\n",
      "\n",
      "Question: What might someone shout when he or she is frustrated?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: C\n",
      "Correct Answer: E\n",
      "\n",
      "Random Choice Model Accuracy: 19.00%\n"
     ]
    }
   ],
   "source": [
    "# Implement random choice model\n",
    "def random_choice_model(question, choices):\n",
    "    return random.choice(choices)\n",
    "\n",
    "# Evaluate the random choice model on 100 sample questions\n",
    "correct_count = 0\n",
    "for i in range(100):\n",
    "    sample = random.choice(train_data)\n",
    "    question = sample['question']\n",
    "    choices = sample['choices']['label']\n",
    "    correct_answer = sample['answerKey']\n",
    "    \n",
    "    # Get the model's prediction\n",
    "    prediction = random_choice_model(question, choices)\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if prediction == correct_answer:\n",
    "        correct_count += 1\n",
    "    if i < 5:  # Print first 5 predictions for verification\n",
    "        print(f\"Question: {question}\\nChoices: {choices}\\nPredicted Answer: {prediction}\\nCorrect Answer: {correct_answer}\\n\")\n",
    "\n",
    "# Calculate and print the accuracy\n",
    "accuracy = correct_count / 100\n",
    "print(f\"Random Choice Model Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f354d7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Where might a milking machine be located?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: A\n",
      "Correct Answer: C\n",
      "\n",
      "Question: The dog had the sheep getting in line, it was amazing a dog could have what skills?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: A\n",
      "Correct Answer: E\n",
      "\n",
      "Question: The parents thought their children should learn teamwork, what were they signed up for?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: A\n",
      "Correct Answer: B\n",
      "\n",
      "Question: What is a person dressed as a clown hoping to accomplish?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: A\n",
      "Correct Answer: B\n",
      "\n",
      "Question: What would an adult woman do to get ready for work?\n",
      "Choices: ['A', 'B', 'C', 'D', 'E']\n",
      "Predicted Answer: A\n",
      "Correct Answer: E\n",
      "\n",
      "Fixed Choice Model Accuracy: 15.00%\n"
     ]
    }
   ],
   "source": [
    "# Implement a fixed choice model\n",
    "def fixed_choice_model(question, choices):\n",
    "    # Always return the first choice as the answer\n",
    "    return choices[0]\n",
    "\n",
    "# Evaluate the fixed choice model on 100 sample questions\n",
    "correct_count_fixed = 0\n",
    "for i in range(100):\n",
    "    sample = random.choice(train_data)\n",
    "    question = sample['question']\n",
    "    choices = sample['choices']['label']\n",
    "    correct_answer = sample['answerKey']\n",
    "    \n",
    "    # Get the model's prediction\n",
    "    prediction = fixed_choice_model(question, choices)\n",
    "    \n",
    "    # Check if the prediction is correct\n",
    "    if prediction == correct_answer:\n",
    "        correct_count_fixed += 1\n",
    "    if i < 5:  # Print first 5 predictions for verification\n",
    "        print(f\"Question: {question}\\nChoices: {choices}\\nPredicted Answer: {prediction}\\nCorrect Answer: {correct_answer}\\n\")\n",
    "# Calculate and print the accuracy for the fixed choice model\n",
    "accuracy_fixed = correct_count_fixed / 100\n",
    "print(f\"Fixed Choice Model Accuracy: {accuracy_fixed:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91df4e9a",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "Both the random and fixed-choice models performed poorly on the subset of 100 random questions from the dataset. They both got anywhere between 15-25% of the answers correctly, which is expected. Both of the models have no sort of reasoning behind them, they both make a blind decision. Since there are 5 total choices for each of the questions and only one of the answers is correct, that gives any random choice theoretically a 20% chance of being choice. The same thing can be said for a fixed choice. If you always pick the same index as the answer, there is a theoretical 20% chance that you will guess correctly.\n",
    "\n",
    "The reason why both models didn't score exactly 20% is due to the distribution of correct answers in the sampled questions. In practice, we would need to repeat this experiment thousands, millions, billions, or even more times (infinity) for the accuracy to begin approaching exactly 20%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Prompting with LLMs (25 pts)\n",
    "- Zero-shot and few-shot prompting\n",
    "- Accuracy comparison\n",
    "\n",
    "*For this part of the assignment, I chose to run a local instance of Llama 3.1 8B*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total prompts created: 9741\n",
      "[('Answer the following questions using common sense reasoning.\\nQuestion: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\\nChoices:\\nA) ignore\\nB) enforce\\nC) authoritarian\\nD) yell at\\nE) avoid\\nChoose the correct option (A, B, C, D, or E) respond ONLY with a single letter:', 'A'), ('Answer the following questions using common sense reasoning.\\nQuestion: Sammy wanted to go to where the people were.  Where might he go?\\nChoices:\\nA) race track\\nB) populated areas\\nC) the desert\\nD) apartment\\nE) roadblock\\nChoose the correct option (A, B, C, D, or E) respond ONLY with a single letter:', 'B'), ('Answer the following questions using common sense reasoning.\\nQuestion: To locate a choker not located in a jewelry box or boutique where would you go?\\nChoices:\\nA) jewelry store\\nB) neck\\nC) jewlery box\\nD) jewelry box\\nE) boutique\\nChoose the correct option (A, B, C, D, or E) respond ONLY with a single letter:', 'A'), ('Answer the following questions using common sense reasoning.\\nQuestion: Google Maps and other highway and street GPS services have replaced what?\\nChoices:\\nA) united states\\nB) mexico\\nC) countryside\\nD) atlas\\nE) oceans\\nChoose the correct option (A, B, C, D, or E) respond ONLY with a single letter:', 'D'), ('Answer the following questions using common sense reasoning.\\nQuestion: The fox walked from the city into the forest, what was it looking for?\\nChoices:\\nA) pretty flowers.\\nB) hen house\\nC) natural habitat\\nD) storybook\\nE) dense forest\\nChoose the correct option (A, B, C, D, or E) respond ONLY with a single letter:', 'C')]\n"
     ]
    }
   ],
   "source": [
    "# Create prompts for the LLM to answer\n",
    "prompts = []\n",
    "\n",
    "for question in dataset['train']:\n",
    "    choices = question['choices']['text']\n",
    "    choice_str = \"\\n\".join([f\"{chr(ord('A') + i)}) {choice}\" for i, choice in enumerate(choices)])\n",
    "    prompt = (\n",
    "        f\"Answer the following questions using common sense reasoning.\\n\"\n",
    "        f\"Question: {question['question']}\\n\"\n",
    "        f\"Choices:\\n{choice_str}\\n\"\n",
    "        f\"Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\"\n",
    "    )\n",
    "    prompts.append((prompt, question['answerKey']))\n",
    "\n",
    "print(f\"Total prompts created: {len(prompts)}\")\n",
    "print(prompts[:5])  # Display first 5 prompts for verification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15557a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with LLaMA 3.1 8B: 100%|██████████| 9741/9741 [15:35<00:00, 10.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "Choices:\n",
      "A) ignore\n",
      "B) enforce\n",
      "C) authoritarian\n",
      "D) yell at\n",
      "E) avoid\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: A\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
      "Choices:\n",
      "A) race track\n",
      "B) populated areas\n",
      "C) the desert\n",
      "D) apartment\n",
      "E) roadblock\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: B\n",
      "Correct: B\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
      "Choices:\n",
      "A) jewelry store\n",
      "B) neck\n",
      "C) jewlery box\n",
      "D) jewelry box\n",
      "E) boutique\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: A\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Google Maps and other highway and street GPS services have replaced what?\n",
      "Choices:\n",
      "A) united states\n",
      "B) mexico\n",
      "C) countryside\n",
      "D) atlas\n",
      "E) oceans\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: D\n",
      "Correct: D\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: The fox walked from the city into the forest, what was it looking for?\n",
      "Choices:\n",
      "A) pretty flowers.\n",
      "B) hen house\n",
      "C) natural habitat\n",
      "D) storybook\n",
      "E) dense forest\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: C\n",
      "Correct: C\n",
      "Is Correct: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Implement zero shot prompting\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "\n",
    "# Define model parameters\n",
    "MODEL = \"llama3.1\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that only response with a single letter: A, B, C, D, or E. Do not include explanations.\"\n",
    "# Run inference using Ollama and LLaMA 3.1 8B model, zero-shot evaluation\n",
    "results = []\n",
    "for prompt, correct_answer in tqdm(prompts, desc=\"Evaluating with LLaMA 3.1 8B\"):\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,  # Make sure you've done `ollama pull llama3`\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': SYSTEM_PROMPT},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().upper()\n",
    "        # Assume answer is something like \"A\", or \"A. choice\", extract the first letter\n",
    "        answer_letter = answer[0] if answer and answer[0] in \"ABCDE\" else \"?\"\n",
    "        is_correct = answer_letter == correct_answer\n",
    "        results.append({\n",
    "            \"question\": prompt,\n",
    "            \"predicted\": answer_letter,\n",
    "            \"correct\": correct_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {prompt}\\nError: {e}\")\n",
    "        results.append({\n",
    "            \"question\": prompt,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": correct_answer,\n",
    "            \"is_correct\": False,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        break\n",
    "# Display th efirst 5 results\n",
    "for result in results[:5]:\n",
    "    print(f\"Question: {result['question']}\\nPredicted: {result['predicted']}\\nCorrect: {result['correct']}\\nIs Correct: {result['is_correct']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606a1126",
   "metadata": {},
   "source": [
    "*Screenshot just in case output disappears:*\n",
    "![Zero shot output](images/zero-shot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c48e314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Zero-Shot Accuracy with LLaMA 3.1 on dataset: 65.73%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate zero-shot accuracy\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate accuracy\n",
    "total = len(results)\n",
    "correct = sum(r['is_correct'] for r in results)\n",
    "accuracy = correct / total * 100\n",
    "\n",
    "# Create results DataFrame\n",
    "df_zero_shot = pd.DataFrame(results)\n",
    "df_zero_shot[\"accuracy (%)\"] = [accuracy] * len(df_zero_shot)  # Same value repeated for readability\n",
    "\n",
    "# Show final accuracy\n",
    "print(f\"\\nZero-Shot Accuracy with LLaMA 3.1 on dataset: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d7d216f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating with Few-Shot LLaMA 3.1 8B: 100%|██████████| 9741/9741 [24:52<00:00,  6.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: The sanctions against the school were a punishing blow, and they seemed to what the efforts the school had made to change?\n",
      "Choices:\n",
      "A) ignore\n",
      "B) enforce\n",
      "C) authoritarian\n",
      "D) yell at\n",
      "E) avoid\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: A\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Sammy wanted to go to where the people were.  Where might he go?\n",
      "Choices:\n",
      "A) race track\n",
      "B) populated areas\n",
      "C) the desert\n",
      "D) apartment\n",
      "E) roadblock\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: B\n",
      "Correct: B\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: To locate a choker not located in a jewelry box or boutique where would you go?\n",
      "Choices:\n",
      "A) jewelry store\n",
      "B) neck\n",
      "C) jewlery box\n",
      "D) jewelry box\n",
      "E) boutique\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: A\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Google Maps and other highway and street GPS services have replaced what?\n",
      "Choices:\n",
      "A) united states\n",
      "B) mexico\n",
      "C) countryside\n",
      "D) atlas\n",
      "E) oceans\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: D\n",
      "Correct: D\n",
      "Is Correct: True\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: The fox walked from the city into the forest, what was it looking for?\n",
      "Choices:\n",
      "A) pretty flowers.\n",
      "B) hen house\n",
      "C) natural habitat\n",
      "D) storybook\n",
      "E) dense forest\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: C\n",
      "Correct: C\n",
      "Is Correct: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import ollama\n",
    "\n",
    "# Implement few-shot prompting\n",
    "few_shot_questions = [question for question in random.sample(list(dataset['train']), 3)]\n",
    "MODEL = \"llama3.1\"\n",
    "SYSTEM_PROMPT = \"You are a helpful assistant that only responds with a single letter: A, B, C, D, or E. Do not include explanations. The following are examples of questions and answers to help you understand the task:\"\n",
    "EXAMPLES = []\n",
    "for question in few_shot_questions:\n",
    "    choices = question['choices']['text']\n",
    "    choice_str = \"\\n\".join([f\"{chr(ord('A') + i)}) {choice}\" for i, choice in enumerate(choices)])\n",
    "    example_prompt = (\n",
    "        f\"Question: {question['question']}\\n\"\n",
    "        f\"Choices:\\n{choice_str}\\n\"\n",
    "        f\"Correct Answer: {question['answerKey']}\\n\"\n",
    "    )\n",
    "    EXAMPLES.append(example_prompt)\n",
    "\n",
    "results = []\n",
    "\n",
    "for prompt, correct_answer in tqdm(prompts, desc=\"Evaluating with Few-Shot LLaMA 3.1 8B\"):\n",
    "    try:\n",
    "        full_system = SYSTEM_PROMPT + \"\\n\\n\" + \"\\n\\n\".join(EXAMPLES) + \"\\n\\n\" + prompt\n",
    "        response = ollama.chat(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {'role': 'system', 'content': full_system},\n",
    "                {'role': 'user', 'content': prompt}\n",
    "            ]\n",
    "        )\n",
    "        answer = response['message']['content'].strip().upper()\n",
    "        answer_letter = answer[0] if answer and answer[0] in \"ABCDE\" else \"?\"\n",
    "        is_correct = answer_letter == correct_answer\n",
    "        results.append({\n",
    "            \"question\": prompt,\n",
    "            \"predicted\": answer_letter,\n",
    "            \"correct\": correct_answer,\n",
    "            \"is_correct\": is_correct\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing prompt: {prompt}\\nError: {e}\")\n",
    "        results.append({\n",
    "            \"question\": prompt,\n",
    "            \"predicted\": \"ERROR\",\n",
    "            \"correct\": correct_answer,\n",
    "            \"is_correct\": False,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "        break\n",
    "# Display the first 5 results\n",
    "for result in results[:5]:\n",
    "    print(f\"Question: {result['question']}\\nPredicted: {result['predicted']}\\nCorrect: {result['correct']}\\nIs Correct: {result['is_correct']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b6aa7d",
   "metadata": {},
   "source": [
    "*Screenshot in case output from above cell disappears:*\n",
    "![Few-shot output](images/few-shot.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e5d3958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Few-Shot Accuracy with LLaMA 3.1 on dataset: 72.19%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Evaluate few-shot accuracy\n",
    "total = len(results)\n",
    "correct = sum(r['is_correct'] for r in results)\n",
    "accuracy = correct / total * 100\n",
    "# Create results DataFrame\n",
    "df_few_shot = pd.DataFrame(results)\n",
    "df_few_shot[\"accuracy (%)\"] = [accuracy] * len(df_few_shot)  # Same value repeated for readability\n",
    "# Show final accuracy\n",
    "print(f\"\\nFew-Shot Accuracy with LLaMA 3.1 on dataset: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25047f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Zero-Shot and Few-Shot Prompting:\n",
      "Zero-Shot Accuracy: 65.73%\n",
      "Few-Shot Accuracy: 72.19%\n",
      "\n",
      "Results DataFrame for Zero-Shot Prompting:\n",
      "                                            question predicted correct  \\\n",
      "0  Answer the following questions using common se...         A       A   \n",
      "1  Answer the following questions using common se...         B       B   \n",
      "2  Answer the following questions using common se...         A       A   \n",
      "3  Answer the following questions using common se...         D       D   \n",
      "4  Answer the following questions using common se...         C       C   \n",
      "\n",
      "   is_correct  accuracy (%)  \n",
      "0        True     65.732471  \n",
      "1        True     65.732471  \n",
      "2        True     65.732471  \n",
      "3        True     65.732471  \n",
      "4        True     65.732471  \n",
      "\n",
      "Results DataFrame for Few-Shot Prompting:\n",
      "                                            question predicted correct  \\\n",
      "0  Answer the following questions using common se...         A       A   \n",
      "1  Answer the following questions using common se...         B       B   \n",
      "2  Answer the following questions using common se...         A       A   \n",
      "3  Answer the following questions using common se...         D       D   \n",
      "4  Answer the following questions using common se...         C       C   \n",
      "\n",
      "   is_correct  accuracy (%)  \n",
      "0        True     72.189714  \n",
      "1        True     72.189714  \n",
      "2        True     72.189714  \n",
      "3        True     72.189714  \n",
      "4        True     72.189714  \n",
      "\n",
      "Number of incorrect predictions in Zero-Shot: 3338\n",
      "Number of incorrect predictions in Few-Shot: 2709\n"
     ]
    }
   ],
   "source": [
    "# Compare accuracies of zero-shot and few-shot prompting and report results\n",
    "print(\"\\nComparison of Zero-Shot and Few-Shot Prompting:\")\n",
    "print(f\"Zero-Shot Accuracy: {df_zero_shot['accuracy (%)'].iloc[0]:.2f}%\")\n",
    "print(f\"Few-Shot Accuracy: {df_few_shot['accuracy (%)'].iloc[0]:.2f}%\")\n",
    "print()\n",
    "print(\"Results DataFrame for Zero-Shot Prompting:\")\n",
    "print(df_zero_shot.head())\n",
    "print(\"\\nResults DataFrame for Few-Shot Prompting:\")\n",
    "print(df_few_shot.head())\n",
    "print()\n",
    "print(f\"Number of incorrect predictions in Zero-Shot: {len(df_zero_shot[df_zero_shot['is_correct'] == False])}\")\n",
    "print(f\"Number of incorrect predictions in Few-Shot: {len(df_few_shot[df_few_shot['is_correct'] == False])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e968226f",
   "metadata": {},
   "source": [
    "*Just in case comparing accuracy and results cell disappears, here's a screenshot of the output:*\n",
    "\n",
    "![Comparison of zero-shot vs few-shot](images/comparison.png)\n",
    "\n",
    "It may also be worth noting that the zero-shot prompts took roughly **15 minutes** in executing, while the few-shot prompts took roughly **25 minutes** in executing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Error Analysis (15 pts)\n",
    "- Choose 5 failed examples\n",
    "- Reasoning or knowledge gap analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Incorrect Predictions from Zero-Shot:\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Why might you be found grooming yourself in the mirror on the way out the door?\n",
      "Choices:\n",
      "A) cleanliness\n",
      "B) mistakes\n",
      "C) anxiety\n",
      "D) beauty\n",
      "E) neatness\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: E\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: What is something bad unlikely to be to anyone?\n",
      "Choices:\n",
      "A) exceptional\n",
      "B) virtuous\n",
      "C) advantageous\n",
      "D) strength\n",
      "E) sufficient\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: C\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: When someone isn't ridiculous at all they are what?\n",
      "Choices:\n",
      "A) straightforward\n",
      "B) serious\n",
      "C) sad\n",
      "D) somber\n",
      "E) solemn\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: B\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Grapes are often grown in what sort of location in order to produce a beverage that impairs the senses when inbibed?\n",
      "Choices:\n",
      "A) rows\n",
      "B) bowl of fruit\n",
      "C) winery\n",
      "D) painting\n",
      "E) fruit stand\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: C\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: John was cleaning out the old house.  while there was nothing upstairs, he found a bunch of old stuff somewhere else.  Where did he find stuff?\n",
      "Choices:\n",
      "A) loft\n",
      "B) attic\n",
      "C) museum\n",
      "D) cellar\n",
      "E) waste bin\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: B\n",
      "Correct: D\n",
      "\n",
      "\n",
      "Random Incorrect Predictions from Few-Shot:\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Why would someone be listening?\n",
      "Choices:\n",
      "A) empathy\n",
      "B) thirsty\n",
      "C) hear things\n",
      "D) knowlege\n",
      "E) learning\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: C\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: If you catch your girlfriend lying about seeing another guy, you'll most likely experience what?\n",
      "Choices:\n",
      "A) broken heart\n",
      "B) mistrust\n",
      "C) getting dumped\n",
      "D) being fired\n",
      "E) get caught\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: C\n",
      "Correct: A\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Residents socialized, played dice, and people smoked all at the busy landing of the what?\n",
      "Choices:\n",
      "A) airport\n",
      "B) apartment building\n",
      "C) stairwell\n",
      "D) ocean\n",
      "E) casino\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: A\n",
      "Correct: B\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Where would one find a snake in a swamp?\n",
      "Choices:\n",
      "A) oregon\n",
      "B) mud\n",
      "C) tropical forest\n",
      "D) pet store\n",
      "E) louisiana\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: B\n",
      "Correct: E\n",
      "\n",
      "Question: Answer the following questions using common sense reasoning.\n",
      "Question: Where might I place the lamp so it will help me see better while I do my homework?\n",
      "Choices:\n",
      "A) nearby\n",
      "B) table\n",
      "C) house\n",
      "D) building\n",
      "E) desktop\n",
      "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
      "Predicted: B\n",
      "Correct: E\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show 5 random incorrect predictions from both models\n",
    "print(\"\\nRandom Incorrect Predictions from Zero-Shot:\")\n",
    "incorrect_zero_shot = df_zero_shot[df_zero_shot['is_correct'] == False].sample(5)\n",
    "for index, row in incorrect_zero_shot.iterrows():\n",
    "    print(f\"Question: {row['question']}\\nPredicted: {row['predicted']}\\nCorrect: {row['correct']}\\n\")\n",
    "print(\"\\nRandom Incorrect Predictions from Few-Shot:\")\n",
    "incorrect_few_shot = df_few_shot[df_few_shot['is_correct'] == False].sample(5)\n",
    "for index, row in incorrect_few_shot.iterrows():\n",
    "    print(f\"Question: {row['question']}\\nPredicted: {row['predicted']}\\nCorrect: {row['correct']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc11725",
   "metadata": {},
   "source": [
    "Here is the output from the above cell in case it disappears:\n",
    "\n",
    "<small>\n",
    "Random Incorrect Predictions from Zero-Shot:\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Why might you be found grooming yourself in the mirror on the way out the door?\n",
    "Choices:\n",
    "A) cleanliness\n",
    "B) mistakes\n",
    "C) anxiety\n",
    "D) beauty\n",
    "E) neatness\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: E\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: What is something bad unlikely to be to anyone?\n",
    "Choices:\n",
    "A) exceptional\n",
    "B) virtuous\n",
    "C) advantageous\n",
    "D) strength\n",
    "E) sufficient\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: C\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: When someone isn't ridiculous at all they are what?\n",
    "Choices:\n",
    "A) straightforward\n",
    "B) serious\n",
    "C) sad\n",
    "D) somber\n",
    "E) solemn\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: B\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Grapes are often grown in what sort of location in order to produce a beverage that impairs the senses when inbibed?\n",
    "Choices:\n",
    "A) rows\n",
    "B) bowl of fruit\n",
    "C) winery\n",
    "D) painting\n",
    "E) fruit stand\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: C\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: John was cleaning out the old house.  while there was nothing upstairs, he found a bunch of old stuff somewhere else.  Where did he find stuff?\n",
    "Choices:\n",
    "A) loft\n",
    "B) attic\n",
    "C) museum\n",
    "D) cellar\n",
    "E) waste bin\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: B\n",
    "Correct: D\n",
    "\n",
    "--------------\n",
    "\n",
    "Random Incorrect Predictions from Few-Shot:\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Why would someone be listening?\n",
    "Choices:\n",
    "A) empathy\n",
    "B) thirsty\n",
    "C) hear things\n",
    "D) knowlege\n",
    "E) learning\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: C\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: If you catch your girlfriend lying about seeing another guy, you'll most likely experience what?\n",
    "Choices:\n",
    "A) broken heart\n",
    "B) mistrust\n",
    "C) getting dumped\n",
    "D) being fired\n",
    "E) get caught\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: C\n",
    "Correct: A\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Residents socialized, played dice, and people smoked all at the busy landing of the what?\n",
    "Choices:\n",
    "A) airport\n",
    "B) apartment building\n",
    "C) stairwell\n",
    "D) ocean\n",
    "E) casino\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: A\n",
    "Correct: B\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Where would one find a snake in a swamp?\n",
    "Choices:\n",
    "A) oregon\n",
    "B) mud\n",
    "C) tropical forest\n",
    "D) pet store\n",
    "E) louisiana\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: B\n",
    "Correct: E\n",
    "\n",
    "Question: Answer the following questions using common sense reasoning.\n",
    "Question: Where might I place the lamp so it will help me see better while I do my homework?\n",
    "Choices:\n",
    "A) nearby\n",
    "B) table\n",
    "C) house\n",
    "D) building\n",
    "E) desktop\n",
    "Choose the correct option (A, B, C, D, or E) respond ONLY with a single letter:\n",
    "Predicted: B\n",
    "Correct: E\n",
    "</small>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ace93",
   "metadata": {},
   "source": [
    "### Zero-shot Analysis\n",
    "Question 1: This question is asking why someone would groom themselves on the way out the door. The LLM answered A (cleanliness) when the correct answer was E (neatness). The LLM likely got this problem wrong due to how similar in meaning the two answers \"cleaniness\" and \"neatness\" are. On top of that, the LLM may be missing knowledge on human behavior such as grooming oneself.\n",
    "\n",
    "Question 2: This question asks what something bad is unlikely to be to anyone, to which the LLM answered A (exceptional) when the correct answer was C (advantageous). The LLM likely also got this wrong due to how similar the answer was to its guess. On top of that, I believe the question required hidden knowledge that something bad may actually be exceptional for someone else.\n",
    "\n",
    "Question 3: This question asks when someone is not ridiculous at all, they are what? The LLM answers A (straightforward) when the correct answer was B (serious). The words straightforward and serious are similar, with the difference that when someone is being straightforward they are simply telling the truth. However, when someone is being serious they are acting in a nature that means to not be joking, which the LLM may have overseen.\n",
    "\n",
    "Question 4: This question asks the location of where grapes are grown when made to produce a beverage that impairs the senses (wine). The LLM answered A (rows) when the correct answer was C (winery). The LLM most likely got this question wrong due to missing the hint that the grapes are grown to make wine, and therefor are grown in a winery. It likely got it wrong to do the inability to associate a \"drink that impairs your senses\" with wine.\n",
    "\n",
    "Question 5: This qusiton asks where John found a bunch of old stuff when cleaning out an old house. The LLM answered B (attic) when the correct answer was D (cellar). The reason the LLM most likely got this question wrong is due to the fact that the question states John found nothing upstairs, which means that there is a second story in the house and the attic is in the second story. Therefor, there was also nothing in the attic. The only other logical answer would be the cellar, the correct answer.\n",
    "\n",
    "### Few-shot Analysis\n",
    "Question 1: This question asks why would someone be listening. The LLM answers A (empathy) when the correct answer was C (hear things). The LLM likely got this one wrong likely due to it trying to connect emotions to the questions or the examples provided as hints included emotion in them, which led it to choose empathy instead of the straight up answer to hear things.\n",
    "\n",
    "Question 2: This question asks what you would experience after catching your girlfriend lying about seeing another guy. The LLM answered C (getting dumped) when the correct answer was A (broken heart). THe LLM most likely got this one wrong due to its lack of training when dealing with human emotions and most likely associated \"cheating\" with a break up and chose \"getting dumped\". However, in this case you would experience some sort of heart break rather you yourself getting dumped, most liekly.\n",
    "\n",
    "Question 3: This question asks where residents would socialize, smoke, and played dice at the landing of. The LLM answered A (airport) when the correct answer was B (apartment building). THe LLM most likely saw the word \"landing\" and immediately assumed the correct answer was \"airport\", thinking of a plane. But missed the word \"residents\" which implies they are somewhere communal, like an apartment building. More specifically the landing (an area between stairways) of an apartment building.\n",
    "\n",
    "Question 4: This question asks where someone would find a snake in a swamp. The LLM answered B (mud) when the correct answer was E (louisiana). The LLM most likely does not have much context as to what exactly can be found in certain locations, such as states, and went with the next best answer \"mud\". It likely got this wrong due to a lack of specific information on geography and nature.\n",
    "\n",
    "Question 5: This question asks where someone might place a lamp so it will help them see better while they do their homework. The LLM answered B (table) when the correct answer was E (desktop). The LLM most likely got this one wrong as a \"table\" is very broad and could include dinner tables and coffee tables. The LLM likely failed to recognize that homework is more likely to be done on a desktop and not just any table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Report or Video (10 pts)\n",
    "- Report key findings, surprising failures, reflections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321faf5c",
   "metadata": {},
   "source": [
    "### You can find Video here:\n",
    "\n",
    "<iframe src=\"https://drive.google.com/file/d/1m2lVjRWFwTwo4zqWDUoqOgx2_IDeB9J6/preview\" width=\"600\" height=\"480\"></iframe>\n",
    "\n",
    "Or click <a href=\"https://drive.google.com/file/d/1m2lVjRWFwTwo4zqWDUoqOgx2_IDeB9J6/view\">HERE</a>\n",
    "\n",
    "Or here is a direct link:\n",
    "https://drive.google.com/file/d/1m2lVjRWFwTwo4zqWDUoqOgx2_IDeB9J6/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Chain-of-Thought Prompting (+5 pts)\n",
    "- Try CoT on 20 examples\n",
    "- Analyze performance impact\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ll",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
